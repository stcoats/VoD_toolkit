{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stcoats/VoD_toolkit/blob/main/VoD_toolkit_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "485cc3f9"
      },
      "source": [
        "# VoD Toolkit v2\n",
        "\n",
        "This notebook contains code for the collection of content from YouTube and Twitch for analysis of speech and chat from recorded live streams (\"Video on Demand = VoD\").\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f71d738e"
      },
      "source": [
        "### 1. Google Drive\n",
        "\n",
        "To connect to Google Drive, mount it in the Colab session. This will allow you to access files stored in your Drive directly from the notebook and to save outputs directly. You will be prompted to authorize Colab to access your Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9404d5c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWuQFx-eT1gd"
      },
      "source": [
        "###2. YouTube cookies\n",
        "\n",
        "YouTube uses cookies, for example to hinder bots and implement age verification. Some content that you wish to collect may require age verification, so you need to get the YouTube cookies file and make it available to yt-dlp.\n",
        "\n",
        "1.   Search for the browser extension \"Get cookies.txt LOCALLY\". Install it in your browser.\n",
        "\n",
        "<img src=\"https://i.imgur.com/3sIwYFh.png\" width=\"75%\">\n",
        "\n",
        "2.   In the same browser session, go to YouTube and start watching any video.\n",
        "3.   Access the extension by clicking the extensions icon in the browser.\n",
        "4.   Download the .txt file in Netscape format by clicking the \"Export\" button.\n",
        "\n",
        "<img src=\"https://i.imgur.com/ww1AjO5.png\" width=\"50%\">\n",
        "\n",
        "5.   Upload that cookies.txt file to Colab or wherever you are running the notebook.\n",
        "\n",
        "The cookies file expires after a period of time. If you run the yt-dlp script below and see a message that the cookie has expired, re-do the steps above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MR1SLnVFeUC"
      },
      "source": [
        "###3. Install packages\n",
        "\n",
        "Install the necessary Python packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arDPPAL3IK5B"
      },
      "outputs": [],
      "source": [
        "%pip install -U yt-dlp[default,curl_cffi] certifi webvtt-py faster-whisper emoji==0.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Import packages"
      ],
      "metadata": {
        "id": "cVCVgw8eVQCW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1BcaWpdIPvL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import HTML\n",
        "import yt_dlp\n",
        "import glob\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuc00aN42ZKR"
      },
      "source": [
        "###5. Get a YouTube transcript + live_chat\n",
        "\n",
        "Use the code below to retrieve content from YouTube.\n",
        "\n",
        "`download archive`: This keeps track of what files you have downloaded, so you don't download them again in a new session. Note that because Colab sessions are not persistent, you want to make sure that this file is saved locally with each Colab session. The local file should be uploaded to Colab before each download session.\n",
        "\n",
        "`cookiefile`: If you get a \"restricted content\" message, uncomment the cookiefile line. Get the cookie as above, and put it in the default directory. If you still get an error, try going to YouTube in your browser, refreshing the page, getting a new cookie file, and making it available to the session.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dfdegVfIRKR"
      },
      "outputs": [],
      "source": [
        "#URLS = ['https://www.youtube.com/@PewDiePie/streams']  #The YT address with all the recorded streams\n",
        "URLS = ['https://www.youtube.com/watch?v=RrFMFwjNKxU']\n",
        "\n",
        "\n",
        "ydl_opts = {\n",
        "    # Specify the file to keep track of downloaded video IDs\n",
        "    #'download_archive': './downloaded_videos.txt',  #This is your archive of downloaded files.\n",
        "    #'cookiefile': './cookies.txt', #This is the cookies file you got from YouTube\n",
        "    'writesubtitles': True, #We want to get any subtitles uploaded for the video\n",
        "    'writeautomaticsub': True, #We also want ASR subtitles\n",
        "    'subtitleslangs': ['en', 'live_chat'], #We want subtitles in English, as well as the chat file for the stream, if it exists\n",
        "    'nooverwrites': True, #Don't re-do it if we already have it (should be superfluous due to the download archive, above)\n",
        "    'outtmpl': './%(uploader)s/%(upload_date)s--%(id)s--%(title)s.%(ext)s', #Defines the file names for the saved downloads\n",
        "    'skip_download': True, #We are not downloading the actual video -- just the transcripts and chats\n",
        "    'force_write_download_archive': True,\n",
        "    'verbose': False, # Add verbose output for debugging\n",
        "    # Configure yt-dlp to use the node runtime for JavaScript challenges\n",
        "    'js_runtimes': {'node':{}}\n",
        "}\n",
        "\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    error_code = ydl.download(URLS[0])\n",
        "\n",
        "print(\"Download process complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yE9mwsARWVw"
      },
      "source": [
        "###6. Download from Colab to your local drive\n",
        "\n",
        "The data in a Colab environment is non-persistent: once you end the Colab (or time out), it is gone. If you want it locally (i.e. outside of Google Drive), the code below will get the files you have downloaded and put them in a .gz file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import tarfile\n",
        "from google.colab import files\n",
        "\n",
        "# Where yt-dlp wrote files (adjust if your outtmpl differs)\n",
        "BASE = \"./\"\n",
        "\n",
        "# Find a subfolder (e.g., uploader dir) that actually exists\n",
        "subdirs = [p for p in glob.glob(os.path.join(BASE, \"*\")) if os.path.isdir(p)]\n",
        "if not subdirs:\n",
        "    raise FileNotFoundError(f\"No subdirectories found in {BASE}. Check your outtmpl and paths.\")\n",
        "\n",
        "# Option 1: newest subdir\n",
        "target_dir = max(subdirs, key=os.path.getmtime)\n",
        "\n",
        "# Define the path to the download archive file\n",
        "download_archive_path = \"./downloaded_videos.txt\"\n",
        "\n",
        "# Define the final archive name\n",
        "final_archive_name = \"./ytdlp_outputs.tar.gz\"\n",
        "\n",
        "# Create a new tar.gz archive\n",
        "with tarfile.open(final_archive_name, \"w:gz\") as tar:\n",
        "    # Add the target directory to the archive\n",
        "    tar.add(target_dir, arcname=os.path.basename(target_dir))\n",
        "\n",
        "    # Add the download archive file to the archive if it exists\n",
        "    if os.path.exists(download_archive_path):\n",
        "        tar.add(download_archive_path, arcname=os.path.basename(download_archive_path))\n",
        "    else:\n",
        "        print(f\"Warning: Download archive file not found at {download_archive_path}. Skipping.\")\n",
        "\n",
        "# Trigger browser download\n",
        "files.download(final_archive_name)\n",
        "print(\"Archive ready:\", final_archive_name)"
      ],
      "metadata": {
        "id": "K1FSSLPETpL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. Get TwitchDownloaderCLI\n",
        "\n",
        " To get the Twitch chat transcript, we will use the **TwitchDowloaderCLI**\n",
        "\n"
      ],
      "metadata": {
        "id": "mHvnrwF6ZTcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/lay295/TwitchDownloader/releases/download/1.56.2/TwitchDownloaderCLI-1.56.2-Linux-x64.zip\n",
        "!unzip TwitchDownloaderCLI-1.56.2-Linux-x64.zip\n",
        "!sudo chmod +x TwitchDownloaderCLI"
      ],
      "metadata": {
        "id": "VSHPVDuoZJnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8. Get information about a specific video\n",
        "\n",
        "Twitch videos are identified with a numerical code found in the URL for the video.\n",
        "\n",
        "https://www.twitch.tv/videos/2683693815\n",
        "\n",
        "We can retrieve information about the video with this identifier."
      ],
      "metadata": {
        "id": "KoEXZPFUeCrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, json\n",
        "\n",
        "def extract_first_json_object(s: str):\n",
        "    start = s.find(\"{\")\n",
        "    if start == -1:\n",
        "        raise ValueError(\"No JSON object found in output.\")\n",
        "    decoder = json.JSONDecoder()\n",
        "    obj, end = decoder.raw_decode(s[start:])\n",
        "    return obj\n",
        "\n",
        "VOD_ID = \"2685556193\"\n",
        "\n",
        "raw = subprocess.check_output(\n",
        "    [\"./TwitchDownloaderCLI\", \"info\", \"--id\", VOD_ID, \"--format\", \"raw\"],\n",
        "    text=True\n",
        ")\n",
        "\n",
        "info1 = extract_first_json_object(raw)\n",
        "\n",
        "video_name = info1[\"data\"][\"video\"][\"title\"]\n",
        "uploader_display = info1[\"data\"][\"video\"][\"owner\"][\"displayName\"]\n",
        "uploader_login = info1[\"data\"][\"video\"][\"owner\"][\"login\"]\n",
        "\n",
        "print(\"video_name:\", video_name)\n",
        "print(\"uploader:\", uploader_display)\n",
        "print(\"channel_login:\", uploader_login)\n"
      ],
      "metadata": {
        "id": "XU8i7CU6ZaMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###9. Get a Twitch video\n",
        "\n",
        "We will retrieve a 5-minute excerpt from the video, in mp4 format. We define a temporary directory for storing the chunks from the stream so as not to exceed the limits of the default /tmp directory."
      ],
      "metadata": {
        "id": "0gKrJ5YUegNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./twitch_temp\n",
        "\n",
        "# 2. Run the download with the --temp-path flag\n",
        "#!./TwitchDownloaderCLI videodownload -u 2681769485 --temp-path ./twitch_temp -o ./2681769485.mp4\n",
        "# Download as M4A (Highest quality audio stream, no video)\n",
        "# The timing info is in seconds. 600s = 10 minutes | 1200s = 20 minutes. 5400s = 1h30m.\n",
        "#times are in seconds; comment these two lines out to get the entire video\n",
        "#comment this line out to get the best available quality\n",
        "\n",
        "VOD_ID = \"2685556193\"\n",
        "\n",
        "!./TwitchDownloaderCLI videodownload --id {VOD_ID} \\\n",
        "    --beginning 5400 \\\n",
        "    --ending 5700 \\\n",
        "    --temp-path ./twitch_temp \\\n",
        "    -q \"360p\" \\\n",
        "    -o \"./{VOD_ID}_workshop_sample.mp4\""
      ],
      "metadata": {
        "id": "jQ_fzjJ1ZZ5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IA7Md0RZfKkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10. Convert the Twitch video to audio\n",
        "\n",
        "Twitch videos don't have ASR captions. We will make them ourselves from an audio file. To do this, we convert the .mp4 to an audio file with ffmpeg. The video can be analyzed later, if desired."
      ],
      "metadata": {
        "id": "DetRhUJtk8ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq update\n",
        "!apt-get -qq install -y ffmpeg\n",
        "!ffmpeg -y -i \"./{VOD_ID}_workshop_sample.mp4\" -ar 16000 -ac 1 \"./{VOD_ID}_workshop_sample.wav\"\n"
      ],
      "metadata": {
        "id": "os3J55VBeSF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###11. Get the Twitch live chat\n",
        "\n",
        "Now we retrieve the live chat for the same video excerpt."
      ],
      "metadata": {
        "id": "ggM0NsF3lJ-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!./TwitchDownloaderCLI chatdownload --id 2681769485 -o ./2681769485.html\n",
        "\n",
        "!./TwitchDownloaderCLI chatdownload --id {VOD_ID} \\\n",
        "    --beginning 5400 \\\n",
        "    --ending 5700 \\\n",
        "    -o \"./{VOD_ID}_workshop_chat.json\""
      ],
      "metadata": {
        "id": "4ZMhvVHIdW8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###12. Transcribe the Twitch audio\n",
        "\n",
        "The next step is to create a transcript of the speech in the excerpt. We will do this with faster-whisper."
      ],
      "metadata": {
        "id": "jWCSUaEvlVXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from faster_whisper import WhisperModel\n",
        "\n",
        "#We are using int8_float16 to stay under the VRAM limit\n",
        "#change the device to \"cpu\" if no gpu is available. Use a smaller model (\"tiny\",\"small\",\"base\", or \"medium\") if necessary\n",
        "model = WhisperModel(\"large-v3\", device=\"cuda\")#, compute_type=\"int8_float16\")\n",
        "#VOD_ID = \"2681576060\"\n",
        "\n",
        "# 3. Transcribe with a small batch size to keep memory stable\n",
        "segments_generator, info = model.transcribe(\n",
        "    f\"./{VOD_ID}_workshop_sample.wav\",\n",
        "    beam_size=5,\n",
        "    language=\"fr\",\n",
        "    vad_filter=True,\n",
        "    vad_parameters=dict(min_silence_duration_ms=500)\n",
        ")\n",
        "\n",
        "# Convert the generator to a list so it can be iterated multiple times\n",
        "segments = list(segments_generator)\n",
        "\n",
        "print(f\"Detected language '{info.language}'\")\n",
        "\n",
        "for segment in segments:\n",
        "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
      ],
      "metadata": {
        "id": "8Y4882wgdlcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###13. Convert the ASR output to a Pandas dataframe\n",
        "\n",
        "Now we make our excerpt into a data frame."
      ],
      "metadata": {
        "id": "CaCpsc7tmF7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "segments_list = []\n",
        "for segment in segments:\n",
        "    segments_list.append({\"start\": segment.start, \"end\": segment.end, \"text\": segment.text})\n",
        "\n",
        "whisper_df = pd.DataFrame(segments_list)\n",
        "whisper_df1 = whisper_df[[\"start\",\"text\"]]\n",
        "whisper_df1.columns = [\"time\",\"text\"]\n",
        "whisper_df1[\"video_name\"] = video_name\n",
        "whisper_df1[\"uploader\"] = uploader_display\n"
      ],
      "metadata": {
        "id": "TdB1gEPCd7jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_df1"
      ],
      "metadata": {
        "id": "XLoJJvCk-JMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###14. Convert the Twitch live chat from .json to a Pandas dataframe\n",
        "\n",
        "Now let's parse the live chat .json"
      ],
      "metadata": {
        "id": "-qm3SerBmYpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def parse_twitch_chat_json(file_path):\n",
        "    \"\"\"\n",
        "    Parses a Twitch chat log JSON file and returns a DataFrame with\n",
        "    HTML img tags for emotes to match the previous HTML parser's behavior.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): The path to the input .json file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing 'time', 'author', and 'message'.\n",
        "    \"\"\"\n",
        "    # Read the JSON content\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        chat_data = json.load(file)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    # The messages are stored in the 'comments' key\n",
        "    for comment in chat_data.get('comments', []):\n",
        "        # 1. Extract Timestamp (Offset format: HH:MM:SS)\n",
        "        offset_seconds = comment.get('content_offset_seconds', 0)\n",
        "        time_stamp = time.strftime('%H:%M:%S', time.gmtime(offset_seconds))\n",
        "\n",
        "        # 2. Extract Author\n",
        "        author = comment['commenter'].get('display_name', 'Unknown')\n",
        "\n",
        "        # Note: The JSON provided doesn't include the direct URL for badges (only IDs).\n",
        "        # To match your HTML parser perfectly, we stay with the text name.\n",
        "        # If you need badges, they require a separate API call to Twitch.\n",
        "\n",
        "        # 3. Extract Message and Embed Emotes\n",
        "        # We iterate through fragments to reconstruct the message with HTML <img> tags\n",
        "        message_html = \"\"\n",
        "        for fragment in comment.get('message', {}).get('fragments', []):\n",
        "            text = fragment.get('text', \"\")\n",
        "            emoticon = fragment.get('emoticon')\n",
        "\n",
        "            if emoticon:\n",
        "                # Reconstruct the Twitch emote URL using the ID\n",
        "                # Template: https://static-cdn.jtvnw.net/emoticons/v2/{id}/default/dark/1.0\n",
        "                emote_id = emoticon['emoticon_id']\n",
        "                emote_url = f\"https://static-cdn.jtvnw.net/emoticons/v2/{emote_id}/default/dark/1.0\"\n",
        "                message_html += f' <img src=\"{emote_url}\" width=\"20\">'\n",
        "            else:\n",
        "                message_html += text\n",
        "\n",
        "        data.append([time_stamp, author, message_html.strip()])\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data, columns=['time', 'author', 'message'])\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "khfJvtLa3j4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "twitch_df = parse_twitch_chat_json(f\"./{VOD_ID}_workshop_chat.json\")\n",
        "twitch_df[\"video_name\"] = video_name\n",
        "twitch_df[\"uploader\"] = uploader_display\n",
        "\n",
        "# If you are in Jupyter/Colab and want to see the images rendered:\n",
        "from IPython.display import HTML\n",
        "HTML(twitch_df.to_html(escape=False))\n"
      ],
      "metadata": {
        "id": "stUpDiqjOiV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###15. Combine the faster-whisper transcript with the live chat."
      ],
      "metadata": {
        "id": "3gqA7eJ5n2bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "whisper_df1[\"time\"] += 5400  #we retrieved an excerpt beginning at 5400 seconds\n",
        "\n",
        "# --- helper ---\n",
        "def hhmmss_to_seconds(s):\n",
        "    if pd.isna(s):\n",
        "        return pd.NA\n",
        "    h, m, sec = s.split(\":\")\n",
        "    return int(h) * 3600 + int(m) * 60 + float(sec)\n",
        "\n",
        "# --- transcript ---\n",
        "transcript_out = pd.DataFrame({\n",
        "    \"time\": pd.to_numeric(whisper_df1[\"time\"], errors=\"coerce\"),\n",
        "    \"source\": \"transcript\",\n",
        "    \"Content\": whisper_df1[\"text\"],\n",
        "    \"author\": \"\",\n",
        "    \"video_name\": whisper_df1.get(\"video_name\", \"\"),\n",
        "    \"uploader\": whisper_df1.get(\"uploader\", \"\"),\n",
        "})\n",
        "\n",
        "# --- chat ---\n",
        "chat_out = pd.DataFrame({\n",
        "    \"time\": twitch_df[\"time\"].map(hhmmss_to_seconds),\n",
        "    \"source\": \"chat\",\n",
        "    \"Content\": twitch_df[\"message\"],\n",
        "    \"author\": twitch_df[\"author\"],\n",
        "    \"video_name\": twitch_df.get(\"video_name\", \"\"),\n",
        "    \"uploader\": twitch_df.get(\"uploader\", \"\"),\n",
        "})\n",
        "\n",
        "# --- combine ---\n",
        "out = (\n",
        "    pd.concat([transcript_out, chat_out], ignore_index=True)\n",
        "      .dropna(subset=[\"time\"])\n",
        "      .sort_values(\"time\", kind=\"mergesort\")  # stable order\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "out[\"platform\"] = \"Twitch\"\n",
        "\n",
        "out\n"
      ],
      "metadata": {
        "id": "VGL5dlVQn16z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###16. Save the combined output as an HTML file."
      ],
      "metadata": {
        "id": "pv-tZs1CvyBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save `out` to HTML with the exact same CSS/styling as your batch cell\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "# ---- reuse EXACT same CSS string ----\n",
        "CUSTOM_CSS = \"\"\"\n",
        "  body {\n",
        "    font-family: Arial, sans-serif;\n",
        "    margin: 0;\n",
        "  }\n",
        "\n",
        "  .table-wrapper {\n",
        "    width: 100%;\n",
        "    max-width: none;\n",
        "    padding: 20px;\n",
        "    box-sizing: border-box;\n",
        "    overflow-x: auto;         /* allow horizontal scroll if needed */\n",
        "  }\n",
        "\n",
        "  /* stop the title from forcing weird layout */\n",
        "  h2 {\n",
        "    margin: 0 0 12px 0;\n",
        "    font-size: 22px;\n",
        "    line-height: 1.2;\n",
        "    overflow-wrap: anywhere;  /* breaks long tokens */\n",
        "    word-break: break-word;\n",
        "  }\n",
        "\n",
        "  table {\n",
        "    width: 100%;\n",
        "    table-layout: fixed;      /* makes width:100% actually behave predictably */\n",
        "    border-collapse: collapse;\n",
        "    border: 1px solid #ddd;\n",
        "  }\n",
        "\n",
        "  th, td {\n",
        "    border: 1px solid #ddd;\n",
        "    padding: 8px;\n",
        "    text-align: left;\n",
        "    white-space: normal;\n",
        "    overflow-wrap: anywhere;\n",
        "    word-break: break-word;\n",
        "    vertical-align: top;\n",
        "  }\n",
        "\n",
        "  /* DO NOT hide overflow; it makes things look chopped */\n",
        "  td { overflow: visible; }\n",
        "\n",
        "  /* column sizing as percentages so it scales with slide width */\n",
        "  table th:nth-child(1), table td:nth-child(1) { width: 10%; }   /* time */\n",
        "  table th:nth-child(2), table td:nth-child(2) { width: 10%; }   /* source */\n",
        "  table th:nth-child(3), table td:nth-child(3) { width: 10%; }   /* platform */\n",
        "  table th:nth-child(4), table td:nth-child(4) { width: 40%; }  /* content */\n",
        "  table th:nth-child(5), table td:nth-child(5) { width: 10%; }  /* author */\n",
        "  table th:nth-child(6), table td:nth-child(6) { width: 10%; }  /* video_name */\n",
        "  table th:nth-child(7), table td:nth-child(7) { width: 10%; }  /* uploader */\n",
        "\n",
        "\n",
        "  th {\n",
        "    position: sticky;\n",
        "    top: 0;\n",
        "    background-color: #343a40;\n",
        "    color: white;\n",
        "    z-index: 2;\n",
        "  }\n",
        "\n",
        "  tr:nth-child(even) {\n",
        "    background-color: #f2f2f2;\n",
        "  }\n",
        "\n",
        "  td img {\n",
        "    max-height: 24px;\n",
        "    vertical-align: middle;\n",
        "  }\n",
        "\"\"\"\n",
        "\n",
        "# ---- pick a sensible title/filename ----\n",
        "title = \"Combined Data\"\n",
        "if \"video_name\" in out.columns:\n",
        "    vn = str(out[\"video_name\"].iloc[0]) if len(out) else \"\"\n",
        "    if vn:\n",
        "        title = f\"Combined Data for {vn}\"\n",
        "\n",
        "# make a safe filename\n",
        "safe = re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", title)[:180]\n",
        "output_html_path = f\"./{safe}.html\"   # local in your notebook runtime (or change path)\n",
        "os.makedirs(os.path.dirname(output_html_path) or \".\", exist_ok=True)\n",
        "\n",
        "# ensure column order\n",
        "preferred_order = preferred_order = [\"time\", \"source\", \"platform\", \"Content\", \"author\", \"video_name\", \"uploader\"]\n",
        "extras = [c for c in out.columns if c not in preferred_order]\n",
        "out_to_save = out[preferred_order + extras].copy()\n",
        "\n",
        "# IMPORTANT: escape=False so <img> renders (emoji/avatars)\n",
        "pandas_html_table = out_to_save.to_html(\n",
        "    index=False,\n",
        "    render_links=True,\n",
        "    escape=False,\n",
        "    classes=\"dataframe table table-striped\"\n",
        ")\n",
        "\n",
        "full_html_content = f\"\"\"<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\">\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "<title>{title}</title>\n",
        "<link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css\">\n",
        "<style>\n",
        "{CUSTOM_CSS}\n",
        "</style>\n",
        "<script src=\"https://code.jquery.com/jquery-3.5.1.slim.min.js\"></script>\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/sticky-table-headers/js/jquery.stickytableheaders.min.js\"></script>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"table-wrapper\">\n",
        "<h2>{title}</h2>\n",
        "{pandas_html_table}\n",
        "</div>\n",
        "<script>\n",
        "  $(document).ready(function() {{\n",
        "    $(\"table\").stickyTableHeaders();\n",
        "  }});\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "with open(output_html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(full_html_content)\n",
        "\n",
        "print(\"Wrote:\", output_html_path)\n"
      ],
      "metadata": {
        "id": "fB9u5RBsu9-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#YouTube"
      ],
      "metadata": {
        "id": "gDo6SEctfsxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###17. Get pre-collected YouTube material from a Swift container\n",
        "\n",
        "Let's work with some content that has already been downloaded. Retrieve the following file and extract it."
      ],
      "metadata": {
        "id": "F6v2w1U7Y2Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tarfile\n",
        "\n",
        "url = \"https://a3s.fi/swift/v1/Toulouse_Workshop/ytdlp_outputs_pewdiepie.tar.gz\"\n",
        "\n",
        "# 1. Stream the download\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "# 2. Open the stream as a tarfile and extract\n",
        "if response.status_code == 200:\n",
        "    # 'r|gz' tells tarfile to read a gzipped stream\n",
        "    with tarfile.open(fileobj=response.raw, mode=\"r|gz\") as tar:\n",
        "        tar.extractall(path=\"./extracted_data\")\n",
        "        print(\"Extraction complete! Files are in './extracted_data'\")\n",
        "else:\n",
        "    print(f\"Failed to download. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "id": "yfs1IPnDg4cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9072276"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNSd2X56QsM6"
      },
      "source": [
        "###18. Get the VoDs for which we have both the ASR transcript and the live_chat\n",
        "\n",
        "The file `ytdlp_outputs_pewdiepie.tar.gz` has been downloaded and extracted your Colab environment. We can now analyze its contents.\n",
        "\n",
        "For each stream, we attempted to retrieve the live chat as well as the ASR transcript. The live chats are in `.json` format. The transcripts are in `.vtt` format.\n",
        "\n",
        "First, we will make a dictionary of all the videos for which we now have both the transcript and the chat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696ee92b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "extract_path = \"./extracted_data/\"\n",
        "\n",
        "\n",
        "# Process the extracted files\n",
        "\n",
        "base_dir = extract_path\n",
        "file_info = {}\n",
        "\n",
        "# We'll use rglob (recursive glob) to find files in the extracted folders\n",
        "all_files = list(Path(base_dir).rglob(\"*\"))\n",
        "\n",
        "# Get lists of all downloaded json and vtt files\n",
        "all_json_files = glob.glob(os.path.join(base_dir, \"*\", \"*.live_chat.json\"))\n",
        "all_vtt_files = glob.glob(os.path.join(base_dir, \"*\", \"*.en.vtt\")) # Assuming English transcripts\n",
        "\n",
        "# Create a dictionary to store file information, using a cleaned base filename as the key\n",
        "file_info = {}\n",
        "\n",
        "# Function to get a cleaned base name for pairing and extract uploader\n",
        "def get_file_info(filename, base_dir):\n",
        "    filepath = Path(filename)\n",
        "    basename = filepath.name\n",
        "    # Remove .json and .vtt extensions first\n",
        "    name_without_ext = os.path.splitext(basename)[0]\n",
        "    # Remove .live_chat and .en suffixes for pairing\n",
        "    cleaned_basename = name_without_ext\n",
        "    if cleaned_basename.endswith(\".live_chat\"):\n",
        "        cleaned_basename = cleaned_basename[:-len(\".live_chat\")]\n",
        "    elif cleaned_basename.endswith(\".en\"):\n",
        "         cleaned_basename = cleaned_basename[:-len(\".en\")]\n",
        "\n",
        "    # Extract uploader name from the directory path\n",
        "    try:\n",
        "        uploader_dir = filepath.parent\n",
        "        # Ensure uploader_dir is within base_dir\n",
        "        if Path(base_dir) in uploader_dir.parents:\n",
        "             uploader_name = uploader_dir.name\n",
        "        else:\n",
        "             uploader_name = \"Unknown_Uploader\" # Default if not in expected structure\n",
        "    except Exception:\n",
        "        uploader_name = \"Unknown_Uploader\" # Handle potential errors in path manipulation\n",
        "\n",
        "\n",
        "    return cleaned_basename, filename, uploader_name\n",
        "\n",
        "\n",
        "# Populate the dictionary with file information\n",
        "for json_file in all_json_files:\n",
        "    cleaned_basename, filepath, uploader_name = get_file_info(json_file, base_dir)\n",
        "    if cleaned_basename not in file_info:\n",
        "        file_info[cleaned_basename] = {'uploader': uploader_name}\n",
        "    file_info[cleaned_basename]['json'] = filepath\n",
        "\n",
        "for vtt_file in all_vtt_files:\n",
        "    cleaned_basename, filepath, uploader_name = get_file_info(vtt_file, base_dir)\n",
        "    if cleaned_basename not in file_info:\n",
        "         # If a VTT exists without a JSON, still include uploader if extracted\n",
        "        file_info[cleaned_basename] = {'uploader': uploader_name}\n",
        "    file_info[cleaned_basename]['vtt'] = filepath\n",
        "    # Ensure uploader is consistent if both json and vtt exist\n",
        "    if 'uploader' not in file_info[cleaned_basename] or file_info[cleaned_basename]['uploader'] == \"Unknown_Uploader\":\n",
        "         file_info[cleaned_basename]['uploader'] = uploader_name\n",
        "\n",
        "\n",
        "# Filter out entries that don't have both a json and a vtt file\n",
        "complete_file_pairs = {name: info for name, info in file_info.items() if 'json' in info and 'vtt' in info}\n",
        "\n",
        "\n",
        "print(f\"Found {len(all_json_files)} json files and {len(all_vtt_files)} vtt files.\")\n",
        "print(f\"Found {len(complete_file_pairs)} complete json/vtt pairs.\")\n",
        "\n",
        "# Display the complete_file_pairs dictionary to show the paired filenames and uploader\n",
        "display(complete_file_pairs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LWCAuohUWDfM"
      },
      "outputs": [],
      "source": [
        "complete_file_pairs.items()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwZjk7rWVcd_"
      },
      "source": [
        "###19. Function to parse the YouTube .vtt files\n",
        "\n",
        "We want to combine the ASR transcript with the chat. For this, we need to coordinate the files based on timing information.\n",
        "\n",
        "Function to convert the `.vtt` file into a DataFrame with timing information. We are doing some regular expression parsing here to remove suplerfluous repetition of content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsit6e3KZ8mw"
      },
      "outputs": [],
      "source": [
        "def vtt_to_dataframe(vtt_file: str, epsilon: float = 0.15) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Parse VTT like your original code, then drop near-duplicate rows:\n",
        "    same text repeated within `epsilon` seconds (keep the first).\n",
        "    \"\"\"\n",
        "    # Your original VTT processing code starts here\n",
        "    vtt_text = Path(vtt_file).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    hits = re.findall(r\"(?<!<)\\d\\d\\:\\d\\d\\:\\d\\d\\.\\d\\d\\d.*\\n.+?\\n.+?\\n\", vtt_text, flags=re.M)\n",
        "    raw_text = [(x.split(\"\\n\")[0], re.sub(r\"(?:<\\s?\\d\\d\\:\\d\\d\\:\\d\\d\\.\\d\\d\\d>|</?c>)\",\"\",x.split(\"\\n\")[-2])) for x in hits]\n",
        "    text1 = [re.sub(r\" align:start position:0%\",\"\",y[0]) for y in raw_text if not y[1] == \" \"]\n",
        "    text2 = [re.sub(r\" align:start position:0%\",\"\",y[1]) for y in raw_text if not y[1] == \" \"]\n",
        "    text3 = []\n",
        "    for z in zip(text1,text2):\n",
        "        text3.append(z)\n",
        "    transcript_df = pd.DataFrame(text3)\n",
        "    transcript_df.columns = [\"time\",\"text\"]\n",
        "\n",
        "    # --- de-dupe: drop repeats of the same text within epsilon seconds ---\n",
        "    # This part is adapted from the previous vtt_to_dataframe function\n",
        "    def hms_to_s(tline: str) -> float:\n",
        "        start = tline.split(\" --> \")[0]\n",
        "        h, m, s = start.split(\":\")\n",
        "        return int(h)*3600 + int(m)*60 + float(s)\n",
        "\n",
        "    df = pd.DataFrame(text3, columns=[\"time\", \"text\"]) # Use text3 which contains the parsed data\n",
        "    df[\"_s\"] = df[\"time\"].map(hms_to_s)\n",
        "    df[\"_norm\"] = (\n",
        "        df[\"text\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
        "                  .str.strip()\n",
        "                  .str.lower()\n",
        "    )\n",
        "\n",
        "    # sort by time then remove near-duplicates per text\n",
        "    df = df.sort_values(\"_s\").reset_index(drop=True)\n",
        "    gap = df.groupby(\"_norm\")[\"_s\"].diff()          # time since previous same text\n",
        "    keep = gap.isna() | (gap > epsilon)             # keep first or those spaced out\n",
        "    df = df[keep][[\"time\", \"text\"]].reset_index(drop=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv8Axq5PV9NF"
      },
      "source": [
        "###20. Function to parse the YouTube .json files\n",
        "\n",
        "Let's do the same for the chat messages from the stream. This function extracts messages from the chat `.json` file. Here, we are getting the timing information, the author, and the message content, as well as badges and emoji."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FeNE-x2VvNU"
      },
      "outputs": [],
      "source": [
        "def extract_content(input_df):\n",
        "  extracted_messages = []\n",
        "  for entry in input_df['replayChatItemAction']:\n",
        "      #entry1 = json.loads(entry)\n",
        "      actions = entry.get(\"actions\", [])\n",
        "      for action in actions:\n",
        "          item = action.get(\"addChatItemAction\", {}).get(\"item\", {})\n",
        "          renderer = item.get(\"liveChatTextMessageRenderer\", {})\n",
        "          message_runs = renderer.get(\"message\", {}).get(\"runs\", [])\n",
        "          author_name = renderer.get(\"authorName\", {}).get(\"simpleText\", \"\")\n",
        "          timestamp_usec = renderer.get(\"timestampText\", {}).get(\"simpleText\", \"\")\n",
        "          badge_url = ''\n",
        "\n",
        "          try:\n",
        "              # Get author badges (if any)\n",
        "              badges = renderer.get(\"authorBadges\", [])\n",
        "\n",
        "              # Loop through the badges and find the 32px thumbnail\n",
        "              for badge in badges:\n",
        "                  badge_thumbnails = badge.get('liveChatAuthorBadgeRenderer', {}).get('customThumbnail', {}).get('thumbnails', [])\n",
        "                  for thumbnail in badge_thumbnails:\n",
        "                      if thumbnail.get('width') == 32:  # Check if it's a 32-pixel image\n",
        "                          badge_url = thumbnail.get('url', 'No badge')\n",
        "                          break\n",
        "          except (KeyError, IndexError):\n",
        "              badge_url = ''\n",
        "          if badge_url != '':\n",
        "            author_name = author_name + ' <img src=' + badge_url + '>'\n",
        "\n",
        "          # Initialize message content\n",
        "          message_content = []\n",
        "          for run in message_runs:\n",
        "              if \"text\" in run:\n",
        "                  message_content.append(run[\"text\"])\n",
        "              elif \"emoji\" in run:\n",
        "                  emoji_info = run[\"emoji\"]\n",
        "\n",
        "                  if emoji_info.get(\"isCustomEmoji\", False):\n",
        "                      # YouTube/Twitch custom emotes → always images\n",
        "                      thumbnails = emoji_info.get(\"image\", {}).get(\"thumbnails\", [])\n",
        "                      if thumbnails:\n",
        "                          emoji_image = thumbnails[-1].get(\"url\", \"\")\n",
        "                          message_content.append(f'<img src=\"{emoji_image}\" alt=\"custom emoji\">')\n",
        "                  else:\n",
        "                      # Normal Unicode emoji → keep the emoji itself\n",
        "                      emoji_unicode = emoji_info.get(\"emojiId\", \"\")\n",
        "                      if emoji_unicode:\n",
        "                          message_content.append(emoji_unicode)\n",
        "                      else:\n",
        "                          # Fallback: use the accessibility label\n",
        "                          emoji_label = emoji_info.get(\"image\", {}).get(\"accessibility\", {}).get(\"accessibilityData\", {}).get(\"label\", \"\")\n",
        "                          message_content.append(f\":{emoji_label}:\")\n",
        "\n",
        "          # Join all parts of the message (text and emojis)\n",
        "          full_message = \"\".join(message_content)\n",
        "\n",
        "          # Append the extracted data to the list\n",
        "          extracted_messages.append({\n",
        "              \"timestamp_usec\": timestamp_usec,\n",
        "              \"author\": author_name,\n",
        "              \"message\": full_message\n",
        "          })\n",
        "  return extracted_messages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Le63_9IQGvI"
      },
      "source": [
        "###21. Function to normalize timing information for YouTube files\n",
        "\n",
        "We need to convert the timing information in the `.vtt` and the `.json` to the same format. We can use this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hok9OZH7QAg-"
      },
      "outputs": [],
      "source": [
        "\n",
        "_TIME_RE = re.compile(\n",
        "    r'^\\s*(?P<sign>[-+])?\\s*(?:(?P<h>\\d{1,3}):)?(?P<m>\\d{1,2}):(?P<s>\\d{1,2}(?:[.,]\\d{1,6})?)\\s*$'\n",
        ")\n",
        "\n",
        "def string_to_timedelta(time_str):\n",
        "    \"\"\"Parse 'HH:MM:SS(.mmm)' or 'MM:SS(.mmm)' (optionally negative) into seconds (float).\"\"\"\n",
        "    # fast paths\n",
        "    if time_str is None or (isinstance(time_str, float) and pd.isna(time_str)):\n",
        "        return np.nan\n",
        "    if isinstance(time_str, (int, float)):\n",
        "        return float(time_str)\n",
        "\n",
        "    s = str(time_str).strip()\n",
        "    # if it's a cue range, keep the left timestamp\n",
        "    if \" --> \" in s:\n",
        "        s = s.split(\" --> \", 1)[0].strip()\n",
        "\n",
        "    s = s.replace(\"−\", \"-\")          # Unicode minus\n",
        "    m = _TIME_RE.match(s)\n",
        "    if not m:\n",
        "        return np.nan\n",
        "\n",
        "    sign = -1.0 if m.group(\"sign\") == \"-\" else 1.0\n",
        "    h = int(m.group(\"h\") or 0)\n",
        "    m_ = int(m.group(\"m\"))\n",
        "    sec = float(m.group(\"s\").replace(\",\", \".\"))\n",
        "\n",
        "    return sign * (h * 3600 + m_ * 60 + sec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By87tPR3uq0g"
      },
      "source": [
        "###22. Create combined transcript/chat files for each YouTube video\n",
        "\n",
        "Putting it all together: This block applies the functions above to each transcript-chat pair. It outputs each video as as an html file in a new \"processed_htmls\" directory. It also creates a list of all the content which we can make into a large data frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d215cc73"
      },
      "outputs": [],
      "source": [
        "# Process Each Video Pair, Apply Existing Processing, Combine Data, and Save to HTML\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize a list to collect all combined DataFrames\n",
        "all_combined_dfs_list = []\n",
        "\n",
        "# ---- your preferred CSS (paste exactly as requested) ----\n",
        "CUSTOM_CSS = \"\"\"\n",
        "  body {\n",
        "    font-family: Arial, sans-serif;\n",
        "    margin: 0;\n",
        "  }\n",
        "\n",
        "  .table-wrapper {\n",
        "    width: 100%;\n",
        "    max-width: none;\n",
        "    padding: 20px;\n",
        "    box-sizing: border-box;\n",
        "    overflow-x: auto;         /* allow horizontal scroll if needed */\n",
        "  }\n",
        "\n",
        "  /* stop the title from forcing weird layout */\n",
        "  h2 {\n",
        "    margin: 0 0 12px 0;\n",
        "    font-size: 22px;\n",
        "    line-height: 1.2;\n",
        "    overflow-wrap: anywhere;  /* breaks long tokens */\n",
        "    word-break: break-word;\n",
        "  }\n",
        "\n",
        "  table {\n",
        "    width: 100%;\n",
        "    table-layout: fixed;      /* makes width:100% actually behave predictably */\n",
        "    border-collapse: collapse;\n",
        "    border: 1px solid #ddd;\n",
        "  }\n",
        "\n",
        "  th, td {\n",
        "    border: 1px solid #ddd;\n",
        "    padding: 8px;\n",
        "    text-align: left;\n",
        "    white-space: normal;\n",
        "    overflow-wrap: anywhere;\n",
        "    word-break: break-word;\n",
        "    vertical-align: top;\n",
        "  }\n",
        "\n",
        "  /* DO NOT hide overflow; it makes things look chopped */\n",
        "  td { overflow: visible; }\n",
        "\n",
        "  /* column sizing as percentages so it scales with slide width */\n",
        "  table th:nth-child(1), table td:nth-child(1) { width: 10%; }   /* time */\n",
        "  table th:nth-child(2), table td:nth-child(2) { width: 10%; }   /* source */\n",
        "  table th:nth-child(3), table td:nth-child(3) { width: 10%; }   /* platform */\n",
        "  table th:nth-child(4), table td:nth-child(4) { width: 40%; }  /* content */\n",
        "  table th:nth-child(5), table td:nth-child(5) { width: 10%; }  /* author */\n",
        "  table th:nth-child(6), table td:nth-child(6) { width: 10%; }  /* video_name */\n",
        "  table th:nth-child(7), table td:nth-child(7) { width: 10%; }  /* uploader */\n",
        "\n",
        "  th {\n",
        "    position: sticky;\n",
        "    top: 0;\n",
        "    background-color: #343a40;\n",
        "    color: white;\n",
        "    z-index: 2;\n",
        "  }\n",
        "\n",
        "  tr:nth-child(even) {\n",
        "    background-color: #f2f2f2;\n",
        "  }\n",
        "\n",
        "  td img {\n",
        "    max-height: 24px;\n",
        "    vertical-align: middle;\n",
        "  }\n",
        "\"\"\"\n",
        "\n",
        "# Loop through each video pair found in complete_file_pairs\n",
        "for video_name, file_paths in complete_file_pairs.items():\n",
        "    json_path = file_paths[\"json\"]\n",
        "    vtt_path = file_paths[\"vtt\"]\n",
        "    uploader_name = file_paths.get(\"uploader\", \"Unknown_Uploader\")\n",
        "\n",
        "    print(f\"Processing and combining data for video: {video_name} (Uploader: {uploader_name})\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Load and Process Chat Data\n",
        "    # ----------------------------\n",
        "    chat_df_processed = pd.DataFrame()\n",
        "    try:\n",
        "        json_data = pd.read_json(json_path, orient=\"records\", lines=True)\n",
        "        chat_df = pd.DataFrame(extract_content(json_data))\n",
        "\n",
        "        # Convert chat time to timedelta\n",
        "        chat_df[\"time\"] = chat_df[\"timestamp_usec\"].apply(lambda x: string_to_timedelta(x))\n",
        "        chat_df = chat_df.drop(columns=[\"timestamp_usec\"])\n",
        "\n",
        "        # Filter invalid time, add source, platform\n",
        "        chat_df_processed = chat_df.dropna(subset=[\"time\"]).copy()\n",
        "        chat_df_processed[\"source\"] = \"chat\"\n",
        "        chat_df_processed[\"platform\"] = \"YouTube\"\n",
        "\n",
        "        # Ensure expected columns exist\n",
        "        if \"message\" not in chat_df_processed.columns:\n",
        "            chat_df_processed[\"message\"] = \"\"\n",
        "        if \"author\" not in chat_df_processed.columns:\n",
        "            chat_df_processed[\"author\"] = \"\"\n",
        "        if \"platform\" not in chat_df_processed.columns:\n",
        "            chat_df_processed[\"platform\"] = \"\"\n",
        "\n",
        "        # Remove duplicate chat messages (same time, author, message)\n",
        "        chat_df_processed.drop_duplicates(subset=[\"time\", \"author\", \"message\"], inplace=True)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: JSON file not found for {video_name} at {json_path}. Skipping chat processing.\")\n",
        "        chat_df_processed = pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing JSON for {video_name}: {e}\")\n",
        "        chat_df_processed = pd.DataFrame()\n",
        "\n",
        "    # ----------------------------\n",
        "    # Load and Process Transcript Data\n",
        "    # ----------------------------\n",
        "    transcript_df_processed = pd.DataFrame()\n",
        "    try:\n",
        "        transcript_df = vtt_to_dataframe(vtt_path)\n",
        "\n",
        "        # Convert transcript time to timedelta\n",
        "        transcript_df[\"time\"] = transcript_df[\"time\"].apply(lambda x: string_to_timedelta(x))\n",
        "\n",
        "        # Filter invalid time, add source\n",
        "        transcript_df_processed = transcript_df.dropna(subset=[\"time\"]).copy()\n",
        "        transcript_df_processed[\"source\"] = \"transcript\"\n",
        "        transcript_df_processed[\"platform\"] = \"YouTube\"\n",
        "\n",
        "        # Normalize transcript into message\n",
        "        if \"text\" in transcript_df_processed.columns:\n",
        "            transcript_df_processed[\"message\"] = transcript_df_processed[\"text\"]\n",
        "        elif \"message\" not in transcript_df_processed.columns:\n",
        "            transcript_df_processed[\"message\"] = \"\"\n",
        "\n",
        "        # Transcript has no author by default\n",
        "        if \"author\" not in transcript_df_processed.columns:\n",
        "            transcript_df_processed[\"author\"] = \"\"\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: VTT file not found for {video_name} at {vtt_path}. Skipping transcript processing.\")\n",
        "        transcript_df_processed = pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing VTT for {video_name}: {e}\")\n",
        "        transcript_df_processed = pd.DataFrame()\n",
        "\n",
        "    # ----------------------------\n",
        "    # Combine Data for this video\n",
        "    # ----------------------------\n",
        "    combined_df_video = pd.DataFrame()\n",
        "\n",
        "    if (not chat_df_processed.empty) or (not transcript_df_processed.empty):\n",
        "        # Use union of columns so we don't drop anything\n",
        "        all_cols = sorted(set(chat_df_processed.columns) | set(transcript_df_processed.columns))\n",
        "        chat_aligned = chat_df_processed.reindex(columns=all_cols, fill_value=\"\")\n",
        "        transcript_aligned = transcript_df_processed.reindex(columns=all_cols, fill_value=\"\")\n",
        "\n",
        "        combined_df_video = pd.concat([transcript_aligned, chat_aligned], ignore_index=True)\n",
        "\n",
        "        # Sort by time, fill NaN\n",
        "        combined_df_video = combined_df_video.sort_values(by=\"time\").reset_index(drop=True).fillna(\"\")\n",
        "\n",
        "        # Add identifiers\n",
        "        combined_df_video[\"video_name\"] = video_name\n",
        "        combined_df_video[\"uploader\"] = uploader_name\n",
        "\n",
        "        # Rename message -> Content (allows <img> emoji to render)\n",
        "        if \"message\" in combined_df_video.columns:\n",
        "            combined_df_video = combined_df_video.rename(columns={\"message\": \"Content\"})\n",
        "        else:\n",
        "            combined_df_video[\"Content\"] = \"\"\n",
        "\n",
        "        # Drop redundant transcript 'text' if present\n",
        "        if \"text\" in combined_df_video.columns:\n",
        "            combined_df_video = combined_df_video.drop(columns=[\"text\"])\n",
        "\n",
        "        # Put important columns first, keep any extras afterward\n",
        "\n",
        "        preferred_order = [\"time\", \"source\", \"platform\", \"Content\", \"author\", \"video_name\", \"uploader\"]\n",
        "        extras = [c for c in combined_df_video.columns if c not in preferred_order]\n",
        "        combined_df_video = combined_df_video[preferred_order + extras]\n",
        "\n",
        "        # Append for later global concatenation if you use it\n",
        "        all_combined_dfs_list.append(combined_df_video)\n",
        "\n",
        "    else:\n",
        "        print(f\"No data processed for video: {video_name}. Skipping combination and HTML saving.\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Save Individual Combined Data for this video to HTML\n",
        "    # ----------------------------\n",
        "    if not combined_df_video.empty:\n",
        "        output_html_path = f\"/content/processed_htmls/{uploader_name}/{video_name}_combined.html\"\n",
        "        os.makedirs(os.path.dirname(output_html_path), exist_ok=True)\n",
        "\n",
        "        # escape=False so <img> in Content/author renders (custom emoji / avatars)\n",
        "        pandas_html_table = combined_df_video.to_html(\n",
        "            index=False,\n",
        "            render_links=True,\n",
        "            escape=False,\n",
        "            classes=\"dataframe table table-striped\"\n",
        "        )\n",
        "\n",
        "        full_html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\">\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "<title>Combined Data for {video_name}</title>\n",
        "<link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css\">\n",
        "<style>\n",
        "{CUSTOM_CSS}\n",
        "</style>\n",
        "<script src=\"https://code.jquery.com/jquery-3.5.1.slim.min.js\"></script>\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/sticky-table-headers/js/jquery.stickytableheaders.min.js\"></script>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"table-wrapper\">\n",
        "<h2>Combined Data for {video_name}</h2>\n",
        "{pandas_html_table}\n",
        "</div>\n",
        "<script>\n",
        "  $(document).ready(function() {{\n",
        "    $(\"table\").stickyTableHeaders();\n",
        "  }});\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            with open(output_html_path, \"w\", encoding=\"utf-8\") as file:\n",
        "                file.write(full_html_content)\n",
        "            print(f\"Combined data saved to {output_html_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error writing final HTML file for {video_name}: {e}\")\n",
        "    else:\n",
        "        print(f\"No data to save for video: {video_name}\")\n",
        "\n",
        "print(\"\\nFinished processing all video pairs and saving to individual HTML files.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###23. Create a big dataframe with all the YouTube content"
      ],
      "metadata": {
        "id": "cSY58vrmamaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Concatenate all DataFrames collected in the list\n",
        "if all_combined_dfs_list:\n",
        "    total_vids_df = pd.concat(all_combined_dfs_list, ignore_index=True)\n",
        "    # Fill NaN values in the 'author' column with empty strings after loading\n",
        "    total_vids_df['author'] = total_vids_df['author'].fillna('')\n",
        "    print(\"Successfully created total_vids_df from collected DataFrames.\")\n",
        "else:\n",
        "    total_vids_df = pd.DataFrame() # Create an empty DataFrame if no data was processed\n",
        "    print(\"No DataFrames were collected, total_vids_df is empty.\")\n",
        "\n",
        "display(total_vids_df)"
      ],
      "metadata": {
        "id": "tq6ML4xsXs1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###24. Chat/Transcript rows per video"
      ],
      "metadata": {
        "id": "S0kXHbH2azNy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UflDCDUI-JK7"
      },
      "outputs": [],
      "source": [
        "total_vids_df.groupby(['video_name','source'])\\\n",
        "    .agg(messages=('video_name','count')).reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###25. Prepare the content for text analysis\n",
        "\n",
        "Now let's do some basic text analysis. We can use the NLTK library. To start with, we will tokenize the content. For an initial analysis, we will consider just the first four videos in our small corpus."
      ],
      "metadata": {
        "id": "GaZYLUCD4Mw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import html\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Setup\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# 1. Pre-compile Regex (This is much faster than BeautifulSoup)\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def fast_tokenize_and_clean(text):\n",
        "    if not isinstance(text, str) or text == \"\":\n",
        "        return []\n",
        "\n",
        "    # Fast HTML strip and entity conversion (e.g. &amp; -> &)\n",
        "    cleaned = TAG_RE.sub('', text)\n",
        "    cleaned = html.unescape(cleaned)\n",
        "\n",
        "    # Standard NLTK tokenizer\n",
        "    tokens = word_tokenize(cleaned)\n",
        "\n",
        "    # Lowercase and filter empty strings in one pass\n",
        "    return [t.lower() for t in tokens if t.strip()]\n",
        "\n",
        "# 2. Faster Video Name Cleaning\n",
        "def clean_video_name(x, max_len=20):\n",
        "    # split only on -- between word chars (no spaces around it)\n",
        "    parts = re.split(r'(?<=\\w)--(?=\\w)', x)\n",
        "\n",
        "    # expected format: date -- id -- title ...\n",
        "    if len(parts) >= 3:\n",
        "        name = parts[2]\n",
        "    else:\n",
        "        name = parts[-1]\n",
        "\n",
        "    # remove anything except letters/numbers/space\n",
        "    name = re.sub(r'[^A-Za-z0-9\\s]+', '', name).strip()\n",
        "    name = re.sub(r'\\s+', ' ', name)\n",
        "\n",
        "    return (name[:max_len] + \"…\" ) if len(name) > max_len else name\n",
        "\n",
        "# 3. Execution (The Bottleneck)\n",
        "print(\"Processing... (using optimized NLTK + Regex)\")\n",
        "# We use a list comprehension instead of .apply() for a ~20% boost\n",
        "total_vids_df['processed_tokens'] = [fast_tokenize_and_clean(t) for t in total_vids_df['Content']]\n",
        "\n",
        "# 4. Aggregation\n",
        "total_vids_df['word_count'] = total_vids_df['processed_tokens'].apply(len)\n",
        "total_vids_df['video_name'] = total_vids_df['video_name'].apply(clean_video_name)\n",
        "\n",
        "word_counts_df = total_vids_df.groupby(['video_name', 'source'])['word_count'].sum().reset_index()\n",
        "first_4_video_names = word_counts_df['video_name'].unique()[:4]\n",
        "filtered_word_counts_df = word_counts_df[word_counts_df['video_name'].isin(first_4_video_names)].copy()\n",
        "filtered_word_counts_df['video_name_cleaned'] = filtered_word_counts_df['video_name'].apply(clean_video_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "ev0j4Fu50K-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_vids_df"
      ],
      "metadata": {
        "id": "-o5O3aKqHiVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###26. Compare transcript and chat word count per video"
      ],
      "metadata": {
        "id": "lxvcEgi3bBQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Barplot\n",
        "\n",
        "sns.set_theme(font_scale=1.1)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "ax = sns.barplot(\n",
        "    data=filtered_word_counts_df,\n",
        "    x=\"video_name_cleaned\",\n",
        "    y=\"word_count\",\n",
        "    hue=\"source\",\n",
        "    palette=\"Set2\"\n",
        ")\n",
        "\n",
        "ax.set_title(\n",
        "    \"Total Word Count: Transcript vs. Chat Across First 4 Videos\",\n",
        "    fontsize=16,\n",
        "    pad=12\n",
        ")\n",
        "ax.set_xlabel(\"Video\")\n",
        "ax.set_ylabel(\"Word count\")\n",
        "\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "\n",
        "# add labels on bars (same style as Jaccard plot)\n",
        "for p in ax.patches:\n",
        "    h = p.get_height()\n",
        "    if h > 0:\n",
        "        ax.annotate(\n",
        "            f\"{int(h):,}\",\n",
        "            (p.get_x() + p.get_width()/2, h),\n",
        "            ha=\"center\",\n",
        "            va=\"bottom\",\n",
        "            fontsize=10,\n",
        "            xytext=(0, 3),\n",
        "            textcoords=\"offset points\"\n",
        "        )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "weY-3HqD6sNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###27. Vocabulary overlap\n",
        "\n",
        "To what extent does the streamer use the same lexical items as the chat?"
      ],
      "metadata": {
        "id": "PyOj-hkrsMHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "sns.set_theme(font_scale=1.1)\n",
        "\n",
        "# ---------- Video label cleanup (optional, for plotting) ----------\n",
        "def clean_video_name(x, max_len=20):\n",
        "    name = x.split(\"--\")[-1]\n",
        "    name = re.sub(r'[^a-zA-Z0-9\\s]', '', name).strip()\n",
        "    return name[:max_len] + (\"…\" if len(name) > max_len else \"\")\n",
        "\n",
        "# ---------- Token normalization + filtering ----------\n",
        "EMOJI_RE = re.compile(\n",
        "    \"[\"                     # broad emoji block coverage\n",
        "    \"\\U0001F300-\\U0001FAFF\"\n",
        "    \"\\U00002700-\\U000027BF\"\n",
        "    \"\\U00002600-\\U000026FF\"\n",
        "    \"]+\", flags=re.UNICODE\n",
        ")\n",
        "\n",
        "def normalize_token(tok: str) -> str | None:\n",
        "    if tok is None:\n",
        "        return None\n",
        "    t = str(tok).strip().lower()\n",
        "    if not t:\n",
        "        return None\n",
        "\n",
        "    # drop emojis and purely symbolic tokens\n",
        "    if EMOJI_RE.search(t):\n",
        "        return None\n",
        "\n",
        "    # must contain at least one alnum\n",
        "    if not re.search(r\"[a-z0-9]\", t):\n",
        "        return None\n",
        "\n",
        "    # remove obvious punctuation noise (keep internal apostrophes if you want: adjust if needed)\n",
        "    t = re.sub(r\"[^a-z0-9'\\-]+\", \"\", t).strip(\"-'\")\n",
        "\n",
        "    if not t:\n",
        "        return None\n",
        "\n",
        "    # collapse elongations: \"ahhhhh\" -> \"ahh\", \"nooooo\" -> \"noo\"\n",
        "    # (keeps some expressive length but stops infinite variants)\n",
        "    t = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", t)\n",
        "\n",
        "    # drop tokens that are now too short / junky (tune as needed)\n",
        "    if len(t) == 1 and t not in {\"a\", \"i\"}:\n",
        "        return None\n",
        "\n",
        "    # optionally drop common chat fillers (add/remove as you like)\n",
        "    # if t in {\"lol\", \"lmao\", \"haha\", \"uh\", \"um\"}: return None\n",
        "\n",
        "    return t\n",
        "\n",
        "def normalized_tokens(df):\n",
        "    toks = []\n",
        "    for row in df[\"processed_tokens\"]:\n",
        "        for tok in row:\n",
        "            nt = normalize_token(tok)\n",
        "            if nt is not None:\n",
        "                toks.append(nt)\n",
        "    return toks\n",
        "\n",
        "# ---------- Similarity metrics ----------\n",
        "def jaccard_types(tokens_a, tokens_b):\n",
        "    A, B = set(tokens_a), set(tokens_b)\n",
        "    inter = len(A & B)\n",
        "    union = len(A | B)\n",
        "    return (inter / union) if union else np.nan, inter, union, len(A), len(B)\n",
        "\n",
        "def cosine_counts(tokens_a, tokens_b):\n",
        "    ca, cb = Counter(tokens_a), Counter(tokens_b)\n",
        "    keys = set(ca) | set(cb)\n",
        "    if not keys:\n",
        "        return np.nan\n",
        "    dot = sum(ca[k] * cb[k] for k in keys)\n",
        "    na = np.sqrt(sum(v*v for v in ca.values()))\n",
        "    nb = np.sqrt(sum(v*v for v in cb.values()))\n",
        "    return (dot / (na * nb)) if (na and nb) else np.nan\n",
        "\n",
        "# ---------- Choose sources explicitly ----------\n",
        "# Set these to match your actual labels exactly:\n",
        "S1 = \"chat\"\n",
        "S2 = \"transcript\"\n",
        "\n",
        "# sanity check\n",
        "srcs = set(total_vids_df[\"source\"].unique())\n",
        "if S1 not in srcs or S2 not in srcs:\n",
        "    print(\"Available sources:\", sorted(srcs))\n",
        "    raise ValueError(f\"Set S1/S2 to match your data. Missing: {[s for s in [S1,S2] if s not in srcs]}\")\n",
        "\n",
        "# ---------- Compute per-video overlap ----------\n",
        "vids = filtered_word_counts_df[\"video_name\"].unique()\n",
        "\n",
        "rows = []\n",
        "for vid in vids:\n",
        "    sub = total_vids_df[total_vids_df[\"video_name\"] == vid]\n",
        "    if sub.empty:\n",
        "        continue\n",
        "\n",
        "    a_df = sub[sub[\"source\"] == S1]\n",
        "    b_df = sub[sub[\"source\"] == S2]\n",
        "    if a_df.empty or b_df.empty:\n",
        "        continue\n",
        "\n",
        "    toks_a = normalized_tokens(a_df)\n",
        "    toks_b = normalized_tokens(b_df)\n",
        "\n",
        "    jacc, inter, union, sizeA, sizeB = jaccard_types(toks_a, toks_b)\n",
        "    cos = cosine_counts(toks_a, toks_b)\n",
        "\n",
        "    rows.append({\n",
        "        \"video_name\": vid,\n",
        "        \"video_name_cleaned\": clean_video_name(vid),\n",
        "        \"source_1\": S1,\n",
        "        \"source_2\": S2,\n",
        "        \"jaccard_types_norm\": jacc,\n",
        "        \"cosine_counts_norm\": cos,\n",
        "        \"shared_types\": inter,\n",
        "        \"union_types\": union,\n",
        "        \"vocab_types_src1\": sizeA,\n",
        "        \"vocab_types_src2\": sizeB,\n",
        "        \"tokens_src1\": len(toks_a),\n",
        "        \"tokens_src2\": len(toks_b),\n",
        "    })\n",
        "\n",
        "overlap_df = pd.DataFrame(rows).sort_values(\"jaccard_types_norm\", ascending=False)\n",
        "display(overlap_df)\n",
        "\n",
        "# ---------- Plot: Jaccard ----------\n",
        "plt.figure(figsize=(14, 5))\n",
        "ax = sns.barplot(data=overlap_df, x=\"video_name_cleaned\", y=\"jaccard_types_norm\")\n",
        "ax.set_title(f\"Vocabulary overlap (Jaccard on normalized types): {S1} vs {S2}\", fontsize=16, pad=12)\n",
        "ax.set_xlabel(\"Video\")\n",
        "ax.set_ylabel(\"Jaccard (types)\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "\n",
        "for p in ax.patches:\n",
        "    h = p.get_height()\n",
        "    if pd.notna(h):\n",
        "        ax.annotate(f\"{h:.2f}\", (p.get_x() + p.get_width()/2, h),\n",
        "                    ha=\"center\", va=\"bottom\", fontsize=10, xytext=(0, 3),\n",
        "                    textcoords=\"offset points\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------- Plot: Cosine ----------\n",
        "plt.figure(figsize=(14, 5))\n",
        "ax = sns.barplot(data=overlap_df, x=\"video_name_cleaned\", y=\"cosine_counts_norm\")\n",
        "ax.set_title(f\"Lexical overlap (Cosine on normalized token counts): {S1} vs {S2}\", fontsize=16, pad=12)\n",
        "ax.set_xlabel(\"Video\")\n",
        "ax.set_ylabel(\"Cosine similarity (counts)\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "\n",
        "for p in ax.patches:\n",
        "    h = p.get_height()\n",
        "    if pd.notna(h):\n",
        "        ax.annotate(f\"{h:.2f}\", (p.get_x() + p.get_width()/2, h),\n",
        "                    ha=\"center\", va=\"bottom\", fontsize=10, xytext=(0, 3),\n",
        "                    textcoords=\"offset points\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ciQWfyb8lHfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###28. Chat density\n",
        "\n",
        "When are there more or fewer chat messages?"
      ],
      "metadata": {
        "id": "RHWvtVHcsYzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a specific video for message density analysis\n",
        "# Let's take the first video from the list of unique video names in total_vids_df\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "if not total_vids_df.empty:\n",
        "    selected_video_name = total_vids_df['video_name'].unique()[0]\n",
        "    viz_df = total_vids_df[total_vids_df['video_name'] == selected_video_name].copy()\n",
        "    print(f\"Analyzing message density for video: {selected_video_name}\")\n",
        "else:\n",
        "    print(\"total_vids_df is empty, cannot perform message density analysis.\")\n",
        "    viz_df = pd.DataFrame() # Create an empty DataFrame to avoid further errors\n",
        "\n",
        "if not viz_df.empty:\n",
        "    # Convert time to minutes (integer part)\n",
        "    viz_df['time_in_minutes'] = (viz_df['time'] / 60).astype(int)\n",
        "\n",
        "    # Group by time in minutes and source, then count messages\n",
        "    # Unstack to get 'chat' and 'transcript' as separate columns\n",
        "    mer2 = viz_df.groupby(['time_in_minutes', 'source']).size().unstack(fill_value=0).reset_index()\n",
        "\n",
        "    # Rename columns for clarity if 'chat' or 'transcript' don't exist (e.g., if only one source is present)\n",
        "    if 'chat' not in mer2.columns: mer2['chat'] = 0\n",
        "    if 'transcript' not in mer2.columns: mer2['transcript'] = 0\n",
        "\n",
        "    # Melt the DataFrame for plotting with seaborn.lineplot\n",
        "    result_df2 = pd.melt(mer2, id_vars='time_in_minutes', value_vars=['chat', 'transcript'], var_name='Source Type', value_name='Message Count')\n",
        "\n",
        "    # Create the plot\n",
        "    sns.set_theme(style=\"darkgrid\")\n",
        "    sns.set_context(\"talk\")\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    ax = sns.lineplot(data=result_df2, x='time_in_minutes', y='Message Count', hue='Source Type')\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
        "    ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
        "    plt.xlabel('Time in minutes')\n",
        "    plt.ylabel('Message Density')\n",
        "    plt.title(f'Message Density Over Time for: {selected_video_name}')\n",
        "    plt.xticks(fontsize=\"12\", rotation=45)\n",
        "    plt.yticks(fontsize=\"12\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No data to plot message density for the selected video.\")"
      ],
      "metadata": {
        "id": "ixyiUr9oFJ6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0Ju4K0RKewr"
      },
      "source": [
        "##Sentiment analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###29. Set up sentiment analysis\n",
        "\n",
        "We will use a CardiffNLP model trained on Twitter messages. Other possibilities are commented out.\n",
        "\n",
        "NOTE: This is really slow with cpu, but much faster with GPU (or a TPU)."
      ],
      "metadata": {
        "id": "jnvpwM-abhSY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34e4be75"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Choose a pre-trained sentiment analysis model\n",
        "# 'sentiment-analysis' is the task, and you can specify a model name\n",
        "# A common and good performing model is 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "# However, for potential emoji handling, models trained on social media data might be better.\n",
        "# Let's start with a general one and see how it performs, then explore others if needed.\n",
        "# We will use 'cardiffnlp/twitter-roberta-base-sentiment-latest' as it is trained on Twitter data and might handle informal language better.\n",
        "\n",
        "# Load the sentiment analysis pipeline\n",
        "sentiment = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    #model=\"finiteautomata/bertweet-base-sentiment-analysis\",\n",
        "    #model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "print(f\"Sentiment analysis pipeline loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment([\"I love this\", \"🙁☹️\", \"😭\", \"what?\"])"
      ],
      "metadata": {
        "id": "vSYYX0XtNeaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###30. Filter our YouTube data.\n",
        "\n",
        "This sentiment model doesn't like NaN values, empty rows, or unexpected Unicode codepoints. Let's filter the data.  "
      ],
      "metadata": {
        "id": "MEEUteuAs8-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(f\"Original row count: {len(total_vids_df)}\")\n",
        "\n",
        "# 1. Force everything to string and handle NaNs properly\n",
        "# Using a single space \" \" is safer than an empty string \"\" for some tokenizers\n",
        "total_vids_df['Content'] = total_vids_df['Content'].astype(str).fillna(\" \").replace('nan', ' ')\n",
        "\n",
        "# 2. Strip whitespace and remove any remaining \"empty\" rows that are just noise\n",
        "total_vids_df['Content'] = total_vids_df['Content'].str.strip()\n",
        "\n",
        "# 3. CRITICAL: Remove actual empty rows.\n",
        "# A completely empty string can sometimes cause the GPU to receive a 0-length tensor,\n",
        "# which triggers the 'device-side assert' error.\n",
        "total_vids_df = total_vids_df[total_vids_df['Content'] != \"\"].copy()\n",
        "\n",
        "# 4. Filter out any potential non-printable characters or corrupt bytes\n",
        "total_vids_df['Content'] = total_vids_df['Content'].apply(\n",
        "    lambda x: \"\".join(i for i in x if i.isprintable())\n",
        ")\n",
        "\n",
        "print(f\"Cleaned row count: {len(total_vids_df)}\")\n",
        "print(\"Data is now safe for GPU pipeline.\")"
      ],
      "metadata": {
        "id": "ibQTaBk278Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###31. Convert to a dataset and assign sentiment values"
      ],
      "metadata": {
        "id": "a0n0DGCKcw-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from datasets import Dataset\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1) Dataset\n",
        "ds = Dataset.from_pandas(total_vids_df[[\"Content\"]].fillna(\"\"))\n",
        "\n",
        "# 2) Load model + tokenizer properly\n",
        "model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "# 3) Force eager attention (bypasses SDPA CUDA path)\n",
        "if hasattr(model.config, \"attn_implementation\"):\n",
        "    model.config.attn_implementation = \"eager\"\n",
        "\n",
        "# 4) Pipeline (note task name + explicit tokenizer/model)\n",
        "pipe = pipeline(\n",
        "    \"text-classification\",          # use this for these models\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=256,                 # keeps things stable + fast\n",
        "    return_all_scores=False\n",
        ")\n",
        "\n",
        "# 5) Run\n",
        "results = []\n",
        "print(f\"Starting sentiment analysis for {len(ds)} entries...\")\n",
        "\n",
        "for out in tqdm(pipe(KeyDataset(ds, \"Content\"), batch_size=32), total=len(ds)):\n",
        "    results.append(out)\n",
        "\n",
        "# 6) Save\n",
        "total_vids_df[\"sentiment\"] = [r[\"label\"] for r in results]\n",
        "total_vids_df[\"score\"] = [float(r[\"score\"]) for r in results]\n"
      ],
      "metadata": {
        "id": "XSKvMpLRVWX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_vids_df"
      ],
      "metadata": {
        "id": "Rfds4zeaDE2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###32. Assign sentiment to n-second bins\n",
        "\n",
        "This may be better than assigning sentiment to every single row."
      ],
      "metadata": {
        "id": "NhTGa-UndCVH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad9e686a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "BRACKET_RE = re.compile(r'^\\s*\\[.*?\\]\\s*$')\n",
        "TAG_RE = re.compile(r'<[^>]+>')          # removes <img ...> etc.\n",
        "WS_RE = re.compile(r'\\s+')\n",
        "\n",
        "def clean_html(t: str) -> str:\n",
        "    if not isinstance(t, str):\n",
        "        return \"\"\n",
        "    t = TAG_RE.sub(\" \", t)              # drop tags\n",
        "    t = WS_RE.sub(\" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def is_junk(t: str) -> bool:\n",
        "    if not isinstance(t, str):\n",
        "        return True\n",
        "    t = t.strip()\n",
        "    if not t:\n",
        "        return True\n",
        "    if BRACKET_RE.match(t):             # [Music], [Applause]\n",
        "        return True\n",
        "    if len(t) < 4:\n",
        "        return True\n",
        "    if len(t.split()) < 2:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def assign_sentiment_over_time(\n",
        "    df,\n",
        "    sentiment,                   # pass your HF pipeline in\n",
        "    text_col=\"Content\",\n",
        "    time_col=\"time\",\n",
        "    source_col=\"source\",\n",
        "    window=20,\n",
        "    batch_size=64,\n",
        "    max_length=256,\n",
        "    return_all_scores=True\n",
        "):\n",
        "    d = df.copy()\n",
        "\n",
        "    # Clean HTML out of the text (important for chat rows with <img>)\n",
        "    d[text_col] = d[text_col].apply(clean_html)\n",
        "\n",
        "    # drop junk rows after cleaning\n",
        "    d = d[~d[text_col].apply(is_junk)].copy()\n",
        "\n",
        "    # bin time (assumes seconds; if your time is ms, adjust window accordingly)\n",
        "    t0 = d[time_col].min()\n",
        "    d[\"time_bin\"] = ((d[time_col] - t0) // window).astype(int)\n",
        "\n",
        "\n",
        "    # concatenate text within each (source, time_bin)\n",
        "    binned = (\n",
        "        d.groupby([source_col, \"time_bin\"], sort=True)\n",
        "         .agg(\n",
        "             start_time=(time_col, \"min\"),\n",
        "             end_time=(time_col, \"max\"),\n",
        "             text=(text_col, lambda s: \" \".join(s.astype(str)))\n",
        "         )\n",
        "         .reset_index()\n",
        "    )\n",
        "\n",
        "    texts = binned[\"text\"].tolist()\n",
        "\n",
        "    # run sentiment\n",
        "    outputs = sentiment(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_length,\n",
        "        batch_size=batch_size,\n",
        "        return_all_scores=return_all_scores\n",
        "    )\n",
        "\n",
        "    # ---- robust mapping to numeric score ----\n",
        "    POS_KEYS = {\"positive\", \"pos\", \"label_2\", \"2\"}\n",
        "    NEG_KEYS = {\"negative\", \"neg\", \"label_0\", \"0\"}\n",
        "    NEU_KEYS = {\"neutral\", \"neu\", \"label_1\", \"1\"}\n",
        "\n",
        "    def norm_label(lab: str) -> str:\n",
        "        lab = (lab or \"\").strip().lower()\n",
        "        return lab\n",
        "\n",
        "    def to_num(x):\n",
        "        # return_all_scores=True => x is a list of dicts [{'label':..., 'score':...}, ...]\n",
        "        if isinstance(x, list):\n",
        "            scores = {norm_label(d[\"label\"]): float(d[\"score\"]) for d in x}\n",
        "            # pick best matching keys present\n",
        "            pos = 0.0\n",
        "            neg = 0.0\n",
        "            for k, v in scores.items():\n",
        "                if k in POS_KEYS or \"pos\" in k:\n",
        "                    pos = max(pos, v)\n",
        "                if k in NEG_KEYS or \"neg\" in k:\n",
        "                    neg = max(neg, v)\n",
        "            return pos - neg\n",
        "\n",
        "        # top label only\n",
        "        lab = norm_label(x.get(\"label\"))\n",
        "        sc = float(x.get(\"score\", 0.0))\n",
        "\n",
        "        if lab in POS_KEYS or \"pos\" in lab:\n",
        "            return sc\n",
        "        if lab in NEG_KEYS or \"neg\" in lab:\n",
        "            return -sc\n",
        "        if lab in NEU_KEYS or \"neu\" in lab:\n",
        "            return 0.0\n",
        "\n",
        "        # unknown label: treat as neutral\n",
        "        return 0.0\n",
        "\n",
        "    binned[\"sentiment\"] = [to_num(o) for o in outputs]\n",
        "    return binned\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###33. Assign sentiment to the start of the YouTube content"
      ],
      "metadata": {
        "id": "puSrVZFcddjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW = 20  # seconds; try 5, 10, 20\n",
        "\n",
        "sentiment = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
        "    device=0,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=256,\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n",
        "binned = assign_sentiment_over_time(total_vids_df.iloc[:3000], sentiment, window=20)\n",
        "binned"
      ],
      "metadata": {
        "id": "tvGv_He5V3px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binned[binned[\"source\"]==\"transcript\"]"
      ],
      "metadata": {
        "id": "f8xZ_jAiLxSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###34. Plot sentiment for the start of the content"
      ],
      "metadata": {
        "id": "WuKlJpPSdrKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# pivot to wide: rows = time bins, columns = sources\n",
        "wide = (\n",
        "    binned\n",
        "    .pivot_table(index=\"time_bin\", columns=\"source\", values=\"sentiment\", aggfunc=\"mean\")\n",
        "    .sort_index()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# nicer x-axis in seconds\n",
        "wide[\"t_sec\"] = wide[\"time_bin\"] * WINDOW + (WINDOW / 2)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "if \"transcript\" in wide.columns:\n",
        "    plt.plot(wide[\"t_sec\"], wide[\"transcript\"], marker=\"o\", label=\"Transcript\")\n",
        "\n",
        "if \"chat\" in wide.columns:\n",
        "    plt.plot(wide[\"t_sec\"], wide[\"chat\"], marker=\"o\", label=\"Chat\")\n",
        "\n",
        "plt.axhline(0, linewidth=1)\n",
        "plt.title(f\"Sentiment over time (bin = {WINDOW}s)\")\n",
        "plt.xlabel(\"Time (seconds)\")\n",
        "plt.ylabel(\"Sentiment score\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "display(wide.head(10))\n"
      ],
      "metadata": {
        "id": "1Y9vnhWoV3c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###35. Correlation of chat and transcript sentiment"
      ],
      "metadata": {
        "id": "Cvi64lTBb4KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sent_map = {\n",
        "    \"positive\":  1,\n",
        "    \"neutral\":   0,\n",
        "    \"negative\": -1\n",
        "}\n",
        "\n",
        "df = total_vids_df.copy()\n",
        "df[\"sentiment_num\"] = df[\"sentiment\"].map(sent_map)\n",
        "\n",
        "window = 30  # seconds\n",
        "df[\"time_bin\"] = (df[\"time\"] // window).astype(int)\n",
        "\n",
        "agg = (\n",
        "    df.groupby([\"time_bin\", \"source\"])[\"sentiment_num\"]\n",
        "      .mean()\n",
        "      .unstack(\"source\")\n",
        "      .dropna()   # keep bins where both chat & transcript exist\n",
        ")\n",
        "\n",
        "corr = agg[\"chat\"].corr(agg[\"transcript\"])\n",
        "corr\n"
      ],
      "metadata": {
        "id": "ul4mjTD-WlUL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c78dedd"
      },
      "source": [
        "###36. Moving average Type-Token Ratio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "270a10ad"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_theme(font_scale=1.1)\n",
        "\n",
        "def flatten_token_lists(series):\n",
        "    out = []\n",
        "    for x in series:\n",
        "        if isinstance(x, list):\n",
        "            out.extend(x)\n",
        "    return out\n",
        "\n",
        "def mattr(tokens, window=100):\n",
        "    # Moving-Average TTR\n",
        "    if not tokens:\n",
        "        return 0.0\n",
        "    n = len(tokens)\n",
        "    if n <= window:\n",
        "        return len(set(tokens)) / n\n",
        "    ttrs = []\n",
        "    for i in range(0, n - window + 1):\n",
        "        w = tokens[i:i+window]\n",
        "        ttrs.append(len(set(w)) / window)\n",
        "    return sum(ttrs) / len(ttrs)\n",
        "\n",
        "WINDOW = 100  # change (50/100/200) depending on how long your texts are\n",
        "\n",
        "# ---- aggregate tokens by video + source ----\n",
        "agg = (\n",
        "    total_vids_df\n",
        "    .groupby([\"video_name\", \"source\"], as_index=False)[\"processed_tokens\"]\n",
        "    .apply(lambda g: flatten_token_lists(g))\n",
        "    .rename(columns={\"processed_tokens\": \"all_tokens\"})\n",
        ")\n",
        "\n",
        "agg[\"mattr\"] = agg[\"all_tokens\"].apply(lambda toks: mattr(toks, window=WINDOW))\n",
        "agg[\"video_name_cleaned\"] = agg[\"video_name\"].astype(str).str.split(\"--\").str[-1].str.slice(0, 20)\n",
        "\n",
        "display(agg[[\"video_name\", \"source\", \"mattr\"]].sort_values([\"video_name\", \"source\"]).head())\n",
        "\n",
        "# ---- plot ----\n",
        "plt.figure(figsize=(20, 8))\n",
        "ax = sns.barplot(\n",
        "    data=agg,\n",
        "    x=\"video_name_cleaned\",\n",
        "    y=\"mattr\",\n",
        "    hue=\"source\",\n",
        "    palette=\"muted\"\n",
        ")\n",
        "plt.title(f\"MATTR (window={WINDOW}) by Video (Transcript vs Chat)\", fontsize=16)\n",
        "plt.xticks(rotation=45, ha=\"right\", fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.ylabel(\"MATTR\", fontsize=14)\n",
        "plt.xlabel(\"video_name\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01f047b0"
      },
      "source": [
        "###37. Vocabulary Growth Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfb23d08"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import chain\n",
        "import numpy as np\n",
        "\n",
        "sns.set_theme(font_scale=1.1)\n",
        "\n",
        "def vocab_growth(tokens, max_n=None):\n",
        "    unique = set()\n",
        "    xs, ys = [], []\n",
        "    n = len(tokens) if max_n is None else min(len(tokens), max_n)\n",
        "    for i in range(n):\n",
        "        unique.add(tokens[i])\n",
        "        xs.append(i + 1)\n",
        "        ys.append(len(unique))\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "# split by source\n",
        "chat_tokens = list(chain.from_iterable(\n",
        "    total_vids_df.loc[total_vids_df[\"source\"] == \"chat\", \"processed_tokens\"]\n",
        "))\n",
        "transcript_tokens = list(chain.from_iterable(\n",
        "    total_vids_df.loc[total_vids_df[\"source\"] == \"transcript\", \"processed_tokens\"]\n",
        "))\n",
        "\n",
        "# ---- make comparable: truncate to same token count ----\n",
        "N = min(len(chat_tokens), len(transcript_tokens))\n",
        "print(f\"Chat tokens: {len(chat_tokens)}\")\n",
        "print(f\"Transcript tokens: {len(transcript_tokens)}\")\n",
        "print(f\"Comparing first N={N} tokens from each\")\n",
        "\n",
        "x_chat, y_chat = vocab_growth(chat_tokens, max_n=N)\n",
        "x_tr, y_tr = vocab_growth(transcript_tokens, max_n=N)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(x_chat, y_chat, label=\"Chat\", linewidth=2)\n",
        "plt.plot(x_tr, y_tr, label=\"Transcript\", linewidth=2)\n",
        "\n",
        "plt.title(f\"Vocabulary Growth (Comparable): Chat vs Transcript (N={N} tokens)\", fontsize=16, pad=12)\n",
        "plt.xlabel(\"Total Tokens (first N)\")\n",
        "plt.ylabel(\"Unique Tokens\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "409ff8f5"
      },
      "source": [
        "###Zipf's Law\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5358b64d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import collections\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "\n",
        "sns.set_theme(font_scale=1.1)\n",
        "\n",
        "# ---- split tokens by source ----\n",
        "chat_tokens = list(chain.from_iterable(\n",
        "    total_vids_df.loc[total_vids_df[\"source\"] == \"chat\", \"processed_tokens\"]\n",
        "))\n",
        "\n",
        "transcript_tokens = list(chain.from_iterable(\n",
        "    total_vids_df.loc[total_vids_df[\"source\"] == \"transcript\", \"processed_tokens\"]\n",
        "))\n",
        "\n",
        "def zipf_data(tokens):\n",
        "    counter = collections.Counter(tokens)\n",
        "    freqs = np.array([c for _, c in counter.most_common()])\n",
        "    ranks = np.arange(1, len(freqs) + 1)\n",
        "    return ranks, freqs\n",
        "\n",
        "# compute Zipf distributions\n",
        "r_chat, f_chat = zipf_data(chat_tokens)\n",
        "r_tr, f_tr = zipf_data(transcript_tokens)\n",
        "\n",
        "# ---- plot ----\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.scatter(r_chat, f_chat, alpha=0.6, s=10, label=\"Chat\")\n",
        "plt.scatter(r_tr, f_tr, alpha=0.6, s=10, label=\"Transcript\")\n",
        "\n",
        "plt.xscale(\"log\")\n",
        "plt.yscale(\"log\")\n",
        "\n",
        "plt.title(\"Zipf's Law Distribution: Chat vs Transcript\", fontsize=16, pad=12)\n",
        "plt.xlabel(\"Rank (log scale)\", fontsize=12)\n",
        "plt.ylabel(\"Frequency (log scale)\", fontsize=12)\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "13lw90cK5PbCp-CRyBIQPYnU5Jwrl0YcN",
      "authorship_tag": "ABX9TyNyRise0wb8yI2XkO7NWheN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}